{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2qqMAXJ9tI5"
      },
      "outputs": [],
      "source": [
        "#Abrindo arquivo base com os dados (importada manualmente)\n",
        "import pandas as pd\n",
        "df = pd.read_excel(\"/content/DMAE.xlsx\", sheet_name = \"dataset\")\n",
        "df['id'] = range(1, len(df) + 1)\n",
        "df = df[['id'] + [col for col in df.columns if col != 'id']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yG9HY5RhL-ST"
      },
      "outputs": [],
      "source": [
        "df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IrgVfyyWd4kb"
      },
      "outputs": [],
      "source": [
        "df.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XSfG5hNrNExD"
      },
      "outputs": [],
      "source": [
        "#Lista de labels das colunas\n",
        "list(df.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5tNsCy7Ne82"
      },
      "outputs": [],
      "source": [
        "# Quantidade de linhas e quantidade de colunas\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8RHmbiuzHqy3"
      },
      "outputs": [],
      "source": [
        "#Declarando o formato das datas para ANO/MÊS/DIA\n",
        "df['DATA'] = pd.to_datetime(df['DATA'], format=\"%Y-%m-%d\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w88T3GFJfHrr"
      },
      "outputs": [],
      "source": [
        "#Ordenando os dados por ordem cronológica\n",
        "df.sort_values(by=['DATA'], ascending=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dun13ldWNgec"
      },
      "outputs": [],
      "source": [
        "#Padronizando os títulos dos veículos\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('A VOZ DO POVO', 'A VOZ DO POVO')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('AGÊNCIA BRASIL', 'AGÊNCIA BRASIL')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('ALÔ UBERLÂNDIA', 'ALÔ UBERLÂNDIA')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('AMVAP', 'AMVAP')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('Balanço Geral', 'BALANÇO GERAL')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('Balanço Geral - Edição de Sábado', 'BALANÇO GERAL')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('BALANÇO GERAL EDIÇÃO DE SÁBADO', 'BALANÇO GERAL')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('BALANÇO GERAL MANHÃ', 'BALANÇO GERAL')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('Balanço Geral Manhã ', 'BALANÇO GERAL')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('Balanço Geral Manhã', 'BALANÇO GERAL')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('Band – Band Cidade', 'BAND CIDADE')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('Band – Bora Minas', 'BORA MINAS')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('Band – Minas Urgente', 'MINAS URGENTE')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('Band – Minas Urgente ', 'MINAS URGENTE')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('BAND CIDADE', 'BAND CIDADE')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('Band Cidade ', 'BAND CIDADE')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('BAND MINAS URGENTE', 'MINAS URGENTE')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('Band Mulher', 'BAND MULHER')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('BORA MINAS', 'BORA MINAS')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('Bora Minas ', 'BORA MINAS')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('BRASIL 61', 'BRASIL 61')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('BRASIL EM DESTAQUE', 'BRASIL EM DESTAQUE')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('CÂMARA MUNICIPAL DE UBERLÂNDIA', 'CÂMARA MUNICIPAL DE UBERLÂNDIA')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('CANAL CANA', 'CANAL DA CANA')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('CANAL TERRA VIVA - JORNAL DA TERRA', 'JORNAL TERRAVIVA')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('Chumbo Grosso', 'CHUMBO GROSSO')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('Chumbo Grosso ', 'CHUMBO GROSSO')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('Chumbo Grosso 2ª Ed.', 'CHUMBO GROSSO')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('CICLO VIVO', 'CICLO VIVO')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('Cidade Alerta Minas', 'CIDADE ALERTA MINAS')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('Cidade Alerta Minas ', 'CIDADE ALERTA MINAS')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('CONEXÃO MINERAL', 'CONEXÃO MINERAL')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('CORREIO BRAZILIENSE', 'CORREIO BRAZILIENSE')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('CORREIO DE UBERLÂNDIA', 'CORREIO DE UBERLÂNDIA')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('Correio de Uberlândia ', 'CORREIO DE UBERLÂNDIA')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('Diário de Uberlândia ', 'DIÁRIO DE UBERLÂNDIA')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('DIARIO DE UBERLÂNDIA', 'DIÁRIO DE UBERLÂNDIA')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('DIÁRIO DE UBERLÂNDIA', 'DIÁRIO DE UBERLÂNDIA')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('DIARIO DE UBERLÂNDIA ', 'DIÁRIO DE UBERLÂNDIA')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('DIÁRIO DE UBERLÂNDIA - JORNAL IMPRESSO', 'DIÁRIO DE UBERLÂNDIA')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('DIÁRIO DE UBERLÂNDIA IMPRESSO', 'DIÁRIO DE UBERLÂNDIA')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('DIÁRIO DE UBERLÂNDIA SITE', 'DIÁRIO DE UBERLÂNDIA')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('DIÁRIO DE UBERLÂNDIA ', 'DIÁRIO DE UBERLÂNDIA')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('DIÁRIO DO COMÉRCIO', 'DIÁRIO DO COMÉRCIO')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('EDIÇAO DO BRASIL', 'EDIÇÃO DO BRASIL')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('EDIÇÃO DO BRASIL', 'EDIÇÃO DO BRASIL')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('Edição do Brasil', 'EDIÇÃO DO BRASIL')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('EDIÇÃO DO BRASIL - IMPRESSO', 'EDIÇÃO DO BRASIL')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('ESTADO DE MINAS', 'ESTADO DE MINAS')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('FOLHA VITÓRIA', 'FOLHA VITÓRIA')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('G1', 'G1')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('G1 ', 'G1')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('G1 - TRIÂNGULO', 'G1 TRIÂNGULO')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('GLOBOPLAY : TV INTEGRAÇÃO - MG1', 'MG 1')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('Hoje em Dia', 'HOJE EM DIA')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('INTEGRAÇÃO NOTICÍA', 'INTEGRAÇÃO NOTÍCIA')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('Integração Notícia', 'INTEGRAÇÃO NOTÍCIA')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('Integração Notícia ', 'INTEGRAÇÃO NOTÍCIA')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('JC - ONLINE', 'JORNAL DO COMMERCIO')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('JM ONLINE', 'JORNAL DA MANHÃ')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('JORNAL CORREIO DA CIDADE - ONLINE', 'CORREIO DA CIDADE')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('JORNAL CORREIO UBERLÂNDIA', 'CORREIO DE UBERLÂNDIA')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('Jornal Floripa', 'JORNAL FLORIPA')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('JORNAL DA UFU', 'JORNAL DA UFU')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('JORNAL IMPRESSO - DIÁRIO DE UBERLÂNDIA', 'DIÁRIO DE UBERLÂNDIA')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('JORNAL IMPRESSO - EDIÇÃO DO BRASIL', 'EDIÇÃO DO BRASIL')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('JORNAL IMPRESSO DIÁRIO DE UBERLÂNDIA', 'DIÁRIO DE UBERLÂNDIA')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('Jornal Paranaíba', 'JORNAL PARANAÍBA')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('Jornal Paranaíba ', 'JORNAL PARANAÍBA')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('Mais Goiás', 'MAIS GOIÁS')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('MANHÃ TOTAL ', 'MANHÃ TOTAL')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('Manhã total ', 'MANHÃ TOTAL')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('MG 1', 'MG 1')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('MG 1 ', 'MG 1')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('MG 2', 'MG 2')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('MG2', 'MG 2')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('NÃO PERDE NÃO', 'NÃO PERDE, NÃO')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('NOTAS E PROSAS', 'NOTAS E PROSAS')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('Notas e Prosas', 'NOTAS E PROSAS')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('O TEMPO', 'O TEMPO')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('Por dentro de Minas', 'POR DENTRO DE MINAS')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('RÁDIO AMÉRICA', 'RÁDIO AMÉRICA')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('RÁDIO AMÉRICA - BOM DIA AMÉRICA', 'RÁDIO AMÉRICA')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('RADIO EDUCADORA - JORNAL EDUCADORA', 'RÁDIO EDUCADORA')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('RÁDIO EDUCADORA - JORNAL EDUCADORA', 'RÁDIO EDUCADORA')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('RADIO VITORIOSA - CAFÉ DA TARDE', 'RÁDIO VITORIOSA')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('RÁDIO VITORIOSA - CAFÉ DA TARDE', 'RÁDIO VITORIOSA')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('Rádio Vitoriosa – Café da Tarde', 'RÁDIO VITORIOSA')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('RADIO VITORIOSA - MANHÃ VITORIOSA', 'RÁDIO VITORIOSA')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('RÁDIO VITORIOSA - MANHA VITORIOSA', 'RÁDIO VITORIOSA')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('RÁDIO VITORIOSA - MANHÃ VITORIOSA', 'RÁDIO VITORIOSA')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('Rádio Vitoriosa – Manhã Vitoriosa', 'RÁDIO VITORIOSA')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('Rádio Vitoriosa – Manhã Vitoriosa ', 'RÁDIO VITORIOSA')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('RÁDIO VITORIOSA -CAFÉ DA TARDE', 'RÁDIO VITORIOSA')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('REDE TV  - ALERTA REGIONAL', 'ALERTA REGIONAL')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('REDE TV - ALERTA REGIONAL', 'ALERTA REGIONAL')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('REDE TV – ALERTA REGIONAL', 'ALERTA REGIONAL')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('Rede TV - Alerta Regional ', 'ALERTA REGIONAL')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('REDE TV - MANHÃ REGIONAL', 'MANHÃ REGIONAL')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('REDE TV - REALIDADE NO AR', 'REALIDADE NO AR')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('REDE TV -ALERTA REGIONAL', 'ALERTA REGIONAL')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('Rede TV – Alerta Regional', 'ALERTA REGIONAL')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('Rede TV – Alerta Regional ', 'ALERTA REGIONAL')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('REDE TV - TARDE DA GENTE', 'TARDE DA GENTE')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('Rede TV – Tarde da Gente', 'TARDE DA GENTE')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('Rede TV – Tarde da Gente ', 'TARDE DA GENTE')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('REDE TV - TARDE REGIONAL', 'TARDE REGIONAL')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('REGIONALZÃO', 'REGIONALZÃO')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('Regionalzão ', 'REGIONALZÃO')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('REGIONALZÃO - TALK', 'REGIONALZÃO')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('REVISTA CULT', 'REVISTA CULT')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('REVISTA TAE', 'REVISTA TAE')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('SAMBIENTAL', 'SAMBIENTAL')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('SANGRES ONLINE', 'SANGRES')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('SITE BARRA', 'SITEBARRA')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('SOBERANA', 'SOBERANA')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('TV BAND - BALANÇO GERAL', 'BALANÇO GERAL')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('TV BAND - BAND CIDADE', 'BAND CIDADE')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('Band Cidade', 'BAND CIDADE')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('TV BAND - BAND MINAS URGENTE', 'MINAS URGENTE')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('TV BAND - BAND MULHER', 'BAND MULHER')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('TV BAND - BAND URGENTE', 'MINAS URGENTE')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('TV BAND - BORA MINAS', 'BORA MINAS')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('TV BAND - BRASIL URGENTE', 'BRASIL URGENTE')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('TV BAND - MINAS URGENTE', 'MINAS URGENTE')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('TV BAND -BRASIL URGENTE', 'BRASIL URGENTE')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('TV INTEGAÇÃO MG1', 'MG 1')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('TV Integração ', 'TV INTEGRAÇÃO')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('TV INTEGRAÇÃO - INTEGRAÇÃO NOTÍCIA', 'INTEGRAÇÃO NOTÍCIA')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('TV INTEGRAÇÃO - INTEGRAÇÃO NOTÍCIAS', 'INTEGRAÇÃO NOTÍCIA')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('TV Integração - Integração Notícias ', 'INTEGRAÇÃO NOTÍCIA')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('TV INTEGRAÇÃO - MG 1', 'MG 1')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('TV INTEGRAÇÃO - MG 2', 'MG 2')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('TV INTEGRAÇÃO - MG1', 'MG 1')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('TV Integração – MG1', 'MG 1')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('TV integração - MG1 ', 'MG 1')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('TV INTEGRAÇÃO - MG2', 'MG 2')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('TV Integração – MG2', 'MG 2')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('TV Integração – MG2 ', 'MG 2')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('TV PARANAÍBA - BALANÇO GERAL', 'TV PARANAÍBA - BALANÇO GERAL')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('TV PARANAÍBA – BALANÇO GERAL', 'TV PARANAÍBA - BALANÇO GERAL')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('TV PARANAÍBA - BALANÇO GERAL ', 'TV PARANAÍBA - BALANÇO GERAL')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('TV Paranaíba – Balanço Geral ', 'TV PARANAÍBA - BALANÇO GERAL')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('TV PARANAÍBA - BALANÇO GERAL  ', 'TV PARANAÍBA - BALANÇO GERAL')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('TV PARANAÍBA - BALANÇO GERAL - MANHÃ', 'TV PARANAÍBA - BALANÇO GERAL')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('TV PARANAÍBA - BALANÇO GERAL MANHÃ', 'TV PARANAÍBA - BALANÇO GERAL')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('TV PARANAÍBA – BALANÇO GERAL MANHÃ', 'TV PARANAÍBA - BALANÇO GERAL')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('TV PARANAÍBA – BALANÇO GERAL ', 'TV PARANAÍBA - BALANÇO GERAL')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('TV PARANAÍBA - CIDADE ALERTA', 'TV PARANAÍBA - CIDADE ALERTA')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('TV Paranaíba – Cidade Alerta ', 'TV PARANAÍBA - CIDADE ALERTA')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('TV PARANAÍBA - CIDADE ALERTA DE MINAS', 'TV PARANAÍBA - CIDADE ALERTA')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('TV PARANAÍBA - CIDADE ALERTA MINAS', 'TV PARANAÍBA - CIDADE ALERTA')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('TV PARANAÍBA - FACEBOOK', 'TV PARANAÍBA')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('TV PARANAÍBA - JORNAL PARANAÍBA', 'JORNAL PARANAÍBA')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('TV PARANAÍBA - MANHÃ TOTAL', 'TV PARANAIBA - MANHÃ TOTAL')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('TV PARANAÍBA - POLÍTICA CRUZADA', 'POLÍTICA CRUZADA')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('TV Paranaíba - Política Cruzada', 'POLÍTICA CRUZADA')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('TV Paranaíba - Rio com Arte', 'RIO COM ARTE')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('TV Paranaíba - Uberabinha Rio com Arte', 'RIO COM ARTE')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('TV UNIVERSITÁRIO - INFORMATIVO TVU', 'INFORMATIVO TVU')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('TV Vitoriosa - Chumbo Grosso', 'CHUMBO GROSSO')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('TV VITORIOSA - CHUMBO GROSSO 2ª edição', 'CHUMBO GROSSO')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('TV VITORIOSA - CHUMBO GROSSO 2ª EDIÇÃO', 'CHUMBO GROSSO')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('Tv Vitoriosa – Chumbo Grosso', 'CHUMBO GROSSO')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('TV VITORIOSA - CHUMBO GROSSO ', 'CHUMBO GROSSO')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('TV VITORIOSA - CHUMBO GROSSO', 'CHUMBO GROSSO')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('TV VITORIOSA - CHUMBO GROSSO - 2ª EDIÇÃO', 'CHUMBO GROSSO')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('TV VITORIOSA - CHUMBO GROSSO - 2ª Edição', 'CHUMBO GROSSO')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('TV VITORIOSA - CHUMBO GROSSO - 2ª edição', 'CHUMBO GROSSO')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('TV VITORIOSA - CHUMBO GROSSO - 2º EDIÇÃO', 'CHUMBO GROSSO')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('TV Vitoriosa – Chumbo Grosso 2ª Ed', 'CHUMBO GROSSO')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('TV Vitoriosa - Chumbo Grosso 2ª Ed.', 'CHUMBO GROSSO')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('TV Vitoriosa – Chumbo Grosso 2ª Ed.', 'CHUMBO GROSSO')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('TV VITORIOSA - MANHÃ VITORIOSA', 'MANHÃ VITORIOSA')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('TV Vitoriosa – Manhã Vitoriosa', 'MANHÃ VITORIOSA')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('TV Vitoriosa – Manhã Vitoriosa ', 'MANHÃ VITORIOSA')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('Tv Vitoriosa – Manhã Vitoriosa', 'MANHÃ VITORIOSA')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('Tv Vitoriosa – Manhã Vitoriosa ', 'MANHÃ VITORIOSA')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('TV VITORIOSA - NA TELA DA VITORIOSA', 'NA TELA DA VITORIOSA')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('TV Vitoriosa – Na Tela da Vitoriosa', 'NA TELA DA VITORIOSA')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('TV VITORIOSA -CHUMBO GROSSO', 'CHUMBO GROSSO')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('TV VITORIOSA -CHUMBO GROSSO - 2ª edição', 'CHUMBO GROSSO')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('TVINTEGRAÇÃO - INTEGRAÇÃO NOTÍCIA', 'INTEGRAÇÃO NOTÍCIA')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('UBERLÂNDIA HOJE', 'UBERLÂNDIA HOJE')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('Uberlândia Hoje', 'UBERLÂNDIA HOJE')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('Uberlândia hoje', 'UBERLÂNDIA HOJE')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('UNIUBE - ACONTECE NA UNIUBE', 'ACONTECE NA UNIUBE')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('V9', 'V9')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('V9 ', 'V9')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('V9 VITORIOSA', 'V9')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('V9 Vitoriosa', 'V9')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('WEB  - DIÁRIO DE UBERLÂNDIA', 'DIÁRIO DE UBERLÂNDIA')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('WEB - DIÁRIO DE UBERLÂNDIA', 'DIÁRIO DE UBERLÂNDIA')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('Diário de Uberlândia', 'DIÁRIO DE UBERLÂNDIA')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('WEB - DIÁRIO DO COMÉRCIO', 'DIÁRIO DO COMÉRCIO')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('WEB - ESTADO DE MINAS ', 'ESTADO DE MINAS')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('WEB - FOLHA UOL', 'FOLHA UOL')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('WEB - G1', 'G1')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('WEB - G1 - JORNAL NACIONAL', 'JORNAL NACIONAL')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('WEB - G1 TRIÂNGULO', 'G1 TRIÂNGULO')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('WEB - JORNAL 10', 'JORNAL 10')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('WEB - NOTAS E PROSAS', 'NOTAS E PROSAS')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('WEB - RÁDIO EDUCADORA ', 'RÁDIO EDUCADORA')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('WEB - REGIONALZÃO', 'REGIONALZÃO')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('WEB - UBERLÂNDIA HOJE', 'UBERLÂNDIA HOJE')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('WEB - V9', 'V9')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('WEB – V9 ', 'V9')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('WEB G1 - TRIÂNGULO', 'G1 TRIÂNGULO')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('WEB- REGIONALZÃO', 'REGIONALZÃO')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('Regionalzão', 'REGIONALZÃO')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('Bora Minas', 'BORA MINAS')\n",
        "df['VEÍCULO'] = df['VEÍCULO'].replace('Estado de Minas', 'ESTADO DE MINAS')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUB22FOKH6Tr"
      },
      "outputs": [],
      "source": [
        "df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.tail()"
      ],
      "metadata": {
        "id": "wALIWMOKMwo0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Salvando esse dataframe padronizado em uma planilha Excel\n",
        "excel_file_path = 'DMAE_Padronizado.xlsx'\n",
        "df.to_excel(excel_file_path, index=False)"
      ],
      "metadata": {
        "id": "jQ6_879TQb2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4E4SnEHxevDq"
      },
      "outputs": [],
      "source": [
        "#Verificando a quantidade de veículos (dados únicos)\n",
        "contagem_veiculos = df['VEÍCULO'].nunique()\n",
        "print(contagem_veiculos)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Lista de veículos\n",
        "lista_veiculos = sorted(df['VEÍCULO'].unique())\n",
        "print(lista_veiculos)"
      ],
      "metadata": {
        "id": "jTzomOvXo1l7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "je90p7AWavaC"
      },
      "outputs": [],
      "source": [
        "#Gerando um texto único com as manchetes da coluna ASSUNTO e salvando em um único arquivo .txt\n",
        "str_assunto = ' '.join(df['ASSUNTO'])\n",
        "text_file = open('texto_manchetes.txt', 'w')\n",
        "text_file.write(str_assunto)\n",
        "text_file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2XScSd7nbJxD"
      },
      "outputs": [],
      "source": [
        "str_assunto[0:1000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oVbiUm6dbZiK"
      },
      "outputs": [],
      "source": [
        "!pip install unidecode\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNpylU7kbqs0"
      },
      "outputs": [],
      "source": [
        "#Limpando texto da coluna ASSUNTO e criando a coluna ASSUNTO CLEAN\n",
        "import re\n",
        "import string\n",
        "from unidecode import unidecode\n",
        "\n",
        "#Transformando tudo para letras minúsculas\n",
        "df['ASSUNTO CLEAN'] = df['ASSUNTO'].apply(lambda x: x.lower())\n",
        "\n",
        "#Remover acentos\n",
        "df['ASSUNTO CLEAN'] = df['ASSUNTO CLEAN'].apply(lambda x: unidecode(x))\n",
        "\n",
        "#Substituir 'ç' por 'c'\n",
        "df['ASSUNTO CLEAN'] = df['ASSUNTO CLEAN'].apply(lambda x: re.sub('ç', 'c', x))\n",
        "\n",
        "#Removendo pontuações\n",
        "df['ASSUNTO CLEAN'] = df['ASSUNTO CLEAN'].apply(lambda x: re.sub('[%s]' % re.escape(string.punctuation), ' ', x))\n",
        "\n",
        "#Removendo números\n",
        "df['ASSUNTO CLEAN'] = df['ASSUNTO CLEAN'].apply(lambda x: re.sub('\\d+', ' ', x))\n",
        "\n",
        "#Removendo palavras curtas\n",
        "df['ASSUNTO CLEAN'] = df['ASSUNTO CLEAN'].apply(lambda x: ' '.join([word for word in x.split() if len(word) > 2]))\n",
        "\n",
        "#Removendo stopwords\n",
        "stopwords = nltk.corpus.stopwords.words('portuguese')\n",
        "stopwords.append('já')\n",
        "stopwords.append('viu')\n",
        "stopwords.append('vai')\n",
        "stopwords.append('aí')\n",
        "stopwords.append('gente')\n",
        "stopwords.append('não')\n",
        "stopwords.append('aqui')\n",
        "stopwords.append('também')\n",
        "stopwords.append('você')\n",
        "stopwords.append('então')\n",
        "stopwords.append('até')\n",
        "stopwords.append('agora')\n",
        "stopwords.append('ser')\n",
        "stopwords.append('sempre')\n",
        "stopwords.append('ter')\n",
        "stopwords.append('só')\n",
        "stopwords.append('porque')\n",
        "stopwords.append('sobre')\n",
        "stopwords.append('ainda')\n",
        "stopwords.append('lá')\n",
        "stopwords.append('tudo')\n",
        "stopwords.append('ninguém')\n",
        "stopwords.append('de')\n",
        "stopwords.append('para')\n",
        "stopwords.append('pode')\n",
        "stopwords.append('nesta')\n",
        "stopwords.append('neste')\n",
        "stopwords.append('cerca')\n",
        "df['ASSUNTO CLEAN'] = df['ASSUNTO CLEAN'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords)]))\n",
        "\n",
        "#Remoção de caracteres NON-ASCII\n",
        "def remove_non_ascii(words):\n",
        "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "        new_words.append(new_word)\n",
        "    return new_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2ZntkNpbv7Y"
      },
      "outputs": [],
      "source": [
        "df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.tail()"
      ],
      "metadata": {
        "id": "wBgsTGQKNOlz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wuRwwfA_cuiK"
      },
      "outputs": [],
      "source": [
        "#Salvando o dataframe em .csv\n",
        "df[['DATA', 'VEÍCULO', 'ASSUNTO', 'ASSUNTO CLEAN']].to_csv('dados_clean.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OAsBG8GGdAi-"
      },
      "outputs": [],
      "source": [
        "#Gerando um texto único na coluna ASSUNTO CLEAN e salvando em um arquivo\n",
        "str_assuntoclean = ' '.join(df['ASSUNTO CLEAN'][df['ASSUNTO CLEAN'].notnull()])\n",
        "text_file = open('texto_manchetes_clean.txt', 'w')\n",
        "text_file.write(str_assuntoclean)\n",
        "text_file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6oBYshsBerGR"
      },
      "outputs": [],
      "source": [
        "#Quantidade de manchetes por veículos\n",
        "df_manchetes_veiculos = df['VEÍCULO'].groupby(df['VEÍCULO']).count().sort_values(ascending=False).to_frame('Quantidade').reset_index()\n",
        "total_manchetes = df_manchetes_veiculos['Quantidade'].sum()\n",
        "df_manchetes_veiculos['Porcentagem'] = (df_manchetes_veiculos['Quantidade'] / total_manchetes) * 100\n",
        "\n",
        "#Exibir os resultados\n",
        "print(df_manchetes_veiculos[0:50])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJZsAPYNhHUh"
      },
      "outputs": [],
      "source": [
        "#Nuvem de palavras dos veículos\n",
        "d = {}\n",
        "\n",
        "for a, x, z in df_manchetes_veiculos.values:\n",
        "  d[a] = x\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "#x, y = np.ogrid[:1000, :1000]\n",
        "#mask = (x - 500) ** 2 + (y - 500) ** 2 > 480 ** 2\n",
        "#mask = 255 * mask.astype(int)\n",
        "\n",
        "#mask = (x / 200) ** 2 + (y / 150) ** 2 > 1\n",
        "#mask = 255 * mask.astype(int)\n",
        "\n",
        "wordcloud = WordCloud(width = 800, height = 600, background_color='white', colormap='Dark2', mask=None, random_state=2021)\n",
        "wordcloud.generate_from_frequencies(frequencies=d)\n",
        "plt.figure(figsize=(20, 10))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNdu9mrb6CBx"
      },
      "outputs": [],
      "source": [
        "#Gráfico de barras de frequência dos veículos\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "uf = df.groupby(by='VEÍCULO').size()\n",
        "uf = uf.reset_index(name='Frequência')\n",
        "\n",
        "#Para mostrar no gráfico apenas os 25 primeiros\n",
        "uf_top25 = uf.sort_values(by='Frequência', ascending=False).head(25)\n",
        "\n",
        "tamanho_figura = (20, 10)\n",
        "\n",
        "ax = uf_top25.plot(kind='bar', x='VEÍCULO', y='Frequência', figsize=tamanho_figura)\n",
        "ax.set_xticks(range(len(uf_top25)))\n",
        "ax.set_xticklabels(uf_top25['VEÍCULO'], rotation=45, ha='right')\n",
        "plt.title('Frequência por veículo de comunicação')\n",
        "\n",
        "for p in ax.patches:\n",
        "    ax.annotate(str(p.get_height()), (p.get_x() + p.get_width() / 2, p.get_height() * 1.01), ha='center')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wD3k81j_5QsB"
      },
      "outputs": [],
      "source": [
        "#Gráfico para identificar os picos de quantidade de notícias por data (sem filtrar por ano)\n",
        "df['DATA'].groupby([df['DATA'].dt.month, df['DATA'].dt.day]).count().plot(\n",
        "    kind='bar',\n",
        "    figsize=(100, 30))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Gráfico para identificar os picos de quantidade de notícias por data em 2022\n",
        "#Filtrar os dados para 2022\n",
        "df_2022 = df[df['DATA'].dt.year == 2022]\n",
        "\n",
        "df_2022['DATA'].groupby([df_2022['DATA'].dt.month, df_2022['DATA'].dt.day]).count().plot(\n",
        "    kind='bar',\n",
        "    figsize=(100, 30),\n",
        "    title='Picos de notícias em 2022'\n",
        ").set_ylim(0, 20)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZolcEgK05ISJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Gráfico para identificar os picos de quantidade de notícias por data em 2023\n",
        "#Filtrar os dados para 2023\n",
        "df_2023 = df[df['DATA'].dt.year == 2023]\n",
        "\n",
        "df_2023['DATA'].groupby([df_2023['DATA'].dt.month, df_2023['DATA'].dt.day]).count().plot(\n",
        "    kind='bar',\n",
        "    figsize=(100, 30),\n",
        "    title='Picos de notícias em 2023'\n",
        "\n",
        ").set_ylim(0, 30)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TQtzAdh_5LL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4UbSykyhl_4"
      },
      "outputs": [],
      "source": [
        "#Gráfico para identificar os picos - 1º Trimestre de 2022\n",
        "df['DATA'][(df['DATA'].dt.year == 2022) & (df['DATA'].dt.month <= 3)].groupby([df['DATA'].dt.to_period(\"M\").rename(\"(mês/ano)\"), df['DATA'].dt.day.rename(\"dia)\")]).count().plot(\n",
        "    kind='bar',\n",
        "    figsize=(20, 10),\n",
        "    title='1º Trimestre 2022 - Janeiro a Março'\n",
        ").set_ylim(0, 20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84_DBFk9iry0"
      },
      "outputs": [],
      "source": [
        "#Gráfico para identificar os picos - 2º Trimestre de 2022\n",
        "df['DATA'][(df['DATA'].dt.year == 2022) & (df['DATA'].dt.month >= 4) & (df['DATA'].dt.month <= 6)].groupby([df['DATA'].dt.to_period(\"M\").rename(\"(mês/ano)\"), df['DATA'].dt.day.rename(\"dia)\")]).count().plot(\n",
        "    kind='bar',\n",
        "    figsize=(20, 10),\n",
        "    title='2º Trimestre de 2022 - Abril a Junho'\n",
        ").set_ylim(0, 20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6Bv8EQNjFhz"
      },
      "outputs": [],
      "source": [
        "#Gráfico para identificar os picos - 3º Trimestre de 2022\n",
        "df['DATA'][(df['DATA'].dt.year == 2022) & (df['DATA'].dt.month >= 7) & (df['DATA'].dt.month <= 9)].groupby([df['DATA'].dt.to_period(\"M\").rename(\"(mês/ano)\"), df['DATA'].dt.day.rename(\"dia)\")]).count().plot(\n",
        "    kind='bar',\n",
        "    figsize=(20, 10),\n",
        "    title='3º Trimestre de 2022 - Julho a Setembro'\n",
        ").set_ylim(0, 20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4GeC0esjHky"
      },
      "outputs": [],
      "source": [
        "#Gráfico para identificar os picos - 4º Trimestre de 2022\n",
        "df['DATA'][(df['DATA'].dt.year == 2022) & (df['DATA'].dt.month >= 10) & (df['DATA'].dt.month <= 12)].groupby([df['DATA'].dt.to_period(\"M\").rename(\"(mês/ano)\"), df['DATA'].dt.day.rename(\"dia)\")]).count().plot(\n",
        "    kind='bar',\n",
        "    figsize=(20, 10),\n",
        "    title='4º Trimestre de 2022 - Outubro a Dezembro'\n",
        ").set_ylim(0, 20)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Gráfico para identificar os picos - 1º Trimestre de 2023\n",
        "df['DATA'][(df['DATA'].dt.year == 2023) & (df['DATA'].dt.month <= 3)].groupby([df['DATA'].dt.to_period(\"M\").rename(\"(mês/ano)\"), df['DATA'].dt.day.rename(\"dia)\")]).count().plot(\n",
        "    kind='bar',\n",
        "    figsize=(20, 10),\n",
        "    title='1º Trimestre 2023 - Janeiro a Março'\n",
        ").set_ylim(0, 20)"
      ],
      "metadata": {
        "id": "dgkiExaR3LRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Gráfico para identificar os picos - 2º Trimestre de 2023\n",
        "df['DATA'][(df['DATA'].dt.year == 2023) & (df['DATA'].dt.month >= 4) & (df['DATA'].dt.month <= 6)].groupby([df['DATA'].dt.to_period(\"M\").rename(\"(mês/ano)\"), df['DATA'].dt.day.rename(\"dia)\")]).count().plot(\n",
        "    kind='bar',\n",
        "    figsize=(20, 10),\n",
        "    title='2º Trimestre de 2023 - Abril a Junho'\n",
        ").set_ylim(0, 20)"
      ],
      "metadata": {
        "id": "FPWbHzB93Lde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Gráfico para identificar os picos - 3º Trimestre de 2023\n",
        "df['DATA'][(df['DATA'].dt.year == 2023) & (df['DATA'].dt.month >= 7) & (df['DATA'].dt.month <= 9)].groupby([df['DATA'].dt.to_period(\"M\").rename(\"(mês/ano)\"), df['DATA'].dt.day.rename(\"dia)\")]).count().plot(\n",
        "    kind='bar',\n",
        "    figsize=(20, 10),\n",
        "    title='3º Trimestre de 2023 - Julho a Setembro'\n",
        ").set_ylim(0, 30)"
      ],
      "metadata": {
        "id": "sPs9Fmmj3LqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6qMiEwnjhP_"
      },
      "outputs": [],
      "source": [
        "#Dias com maior quantidade de notícias\n",
        "df['DATA'].groupby(df['DATA']).count().sort_values(ascending=False)[0:30]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Gráfico de barra para os dias com maior quantidade de notícias\n",
        "top_days = df['DATA'].groupby(df['DATA']).count().sort_values(ascending=False)[0:30]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(20, 10))\n",
        "top_days.plot(kind='bar', ax=ax)\n",
        "plt.title('Dias com Maior Quantidade de Notícias')\n",
        "\n",
        "for p in ax.patches:\n",
        "    ax.annotate(str(p.get_height()), (p.get_x() + p.get_width() / 2, p.get_height() * 1.01), ha='center')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7FT5pjpE8llF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AsQyzMLhkWjG"
      },
      "outputs": [],
      "source": [
        "#Verificando as manchetes dos dias de pico, utilizando o exemplo de 2023-09-26\n",
        "dia_pico = df.loc[df[\"DATA\"] == '2023-09-26', 'ASSUNTO']\n",
        "for headline in dia_pico:\n",
        "    print(headline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eqHoKlZa08j8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4R3HtvlrrEl"
      },
      "outputs": [],
      "source": [
        "#Nuvem de palavras de ASSUNTO CLEAN\n",
        "str_assuntoclean = ' '.join(\n",
        "    df['ASSUNTO CLEAN']\n",
        "    [df['ASSUNTO CLEAN'].notnull()]\n",
        ")\n",
        "\n",
        "str_assuntoclean = str_assuntoclean.replace(' bairros ', ' bairro ')\n",
        "str_assuntoclean = str_assuntoclean.replace(' obras ', ' obra ')\n",
        "str_assuntoclean = str_assuntoclean.replace(' interdicoes ', ' interdicao ')\n",
        "\n",
        "lista_palavras_comuns = ['dmae', 'agua', 'esgoto', 'saneamento', 'basico', 'lixo', 'coleta', 'seletiva', 'cidade', 'bairro', 'local',\n",
        "                         'uberlandia', 'prefeitura', 'avenida', 'rua', 'servico', 'regiao', 'rede', 'iptu',\n",
        "                         'segunda', 'terça', 'quarta', 'quinta', 'sexta', 'sabado', 'domingo', 'feira',\n",
        "                        ]\n",
        "\n",
        "str_assuntoclean = ' '.join(x for x in str_assuntoclean.split() if x not in lista_palavras_comuns)\n",
        "str_assuntoclean = ' '.join(x for x in str_assuntoclean.split())\n",
        "tokens = nltk.word_tokenize(str_assuntoclean)\n",
        "frequencia = nltk.FreqDist(tokens)\n",
        "\n",
        "wordcloud = WordCloud(width = 800, height = 600, background_color='white', colormap='Dark2', mask=None, random_state=2022)\n",
        "wordcloud.generate_from_frequencies(frequencies=frequencia)\n",
        "plt.figure(figsize=(16, 12))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Nuvem de palavras de ASSUNTO CLEAN apenas para 2022\n",
        "df['DATA'] = pd.to_datetime(df['DATA'])\n",
        "df_2022 = df[df['DATA'].dt.year == 2022]\n",
        "\n",
        "str_assuntoclean = ' '.join(\n",
        "    df_2022['ASSUNTO CLEAN']\n",
        "    [df_2022['ASSUNTO CLEAN'].notnull()]\n",
        ")\n",
        "\n",
        "str_assuntoclean = str_assuntoclean.replace(' bairros ', ' bairro ')\n",
        "str_assuntoclean = str_assuntoclean.replace(' obras ', ' obra ')\n",
        "str_assuntoclean = str_assuntoclean.replace(' interdicoes ', ' interdicao ')\n",
        "\n",
        "lista_palavras_comuns = ['dmae', 'agua', 'esgoto', 'saneamento', 'basico', 'lixo', 'coleta', 'seletiva', 'cidade', 'bairro', 'local',\n",
        "                         'uberlandia', 'prefeitura', 'avenida', 'rua', 'servico', 'regiao', 'rede', 'iptu',\n",
        "                         'segunda', 'terça', 'quarta', 'quinta', 'sexta', 'sabado', 'domingo', 'feira',\n",
        "                        ]\n",
        "\n",
        "str_assuntoclean = ' '.join(x for x in str_assuntoclean.split() if x not in lista_palavras_comuns)\n",
        "str_assuntoclean = ' '.join(x for x in str_assuntoclean.split())\n",
        "tokens = nltk.word_tokenize(str_assuntoclean)\n",
        "frequencia = nltk.FreqDist(tokens)\n",
        "\n",
        "wordcloud = WordCloud(width=800, height=600, background_color='white', colormap='Dark2', mask=None, random_state=2022)\n",
        "wordcloud.generate_from_frequencies(frequencies=frequencia)\n",
        "plt.figure(figsize=(16, 12))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iBhQ0CcZD9zh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Nuvem de palavras de ASSUNTO CLEAN apenas para 2023\n",
        "df['DATA'] = pd.to_datetime(df['DATA'])\n",
        "df_2023 = df[df['DATA'].dt.year == 2023]\n",
        "\n",
        "str_assuntoclean = ' '.join(\n",
        "    df_2023['ASSUNTO CLEAN']\n",
        "    [df_2023['ASSUNTO CLEAN'].notnull()]\n",
        ")\n",
        "\n",
        "str_assuntoclean = str_assuntoclean.replace(' bairros ', ' bairro ')\n",
        "str_assuntoclean = str_assuntoclean.replace(' obras ', ' obra ')\n",
        "str_assuntoclean = str_assuntoclean.replace(' interdicoes ', ' interdicao ')\n",
        "\n",
        "lista_palavras_comuns = ['dmae', 'agua', 'esgoto', 'saneamento', 'basico', 'lixo', 'coleta', 'seletiva', 'cidade', 'bairro', 'local',\n",
        "                         'uberlandia', 'prefeitura', 'avenida', 'rua', 'servico', 'regiao', 'rede', 'iptu',\n",
        "                         'segunda', 'terça', 'quarta', 'quinta', 'sexta', 'sabado', 'domingo', 'feira',\n",
        "                        ]\n",
        "\n",
        "str_assuntoclean = ' '.join(x for x in str_assuntoclean.split() if x not in lista_palavras_comuns)\n",
        "str_assuntoclean = ' '.join(x for x in str_assuntoclean.split())\n",
        "tokens = nltk.word_tokenize(str_assuntoclean)\n",
        "frequencia = nltk.FreqDist(tokens)\n",
        "\n",
        "wordcloud = WordCloud(width=800, height=600, background_color='white', colormap='Dark2', mask=None, random_state=2022)\n",
        "wordcloud.generate_from_frequencies(frequencies=frequencia)\n",
        "plt.figure(figsize=(16, 12))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wRLmRLbLEHtv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xojN9vAt2c-d"
      },
      "outputs": [],
      "source": [
        "#Nuvem de palavras do 1º Trimestre de 2022\n",
        "primeirotri = df[(df['DATA'] >= '2022-01-01') & (df['DATA'] <= '2022-03-31')]\n",
        "str_assuntoclean = ' '.join(\n",
        "    primeirotri['ASSUNTO CLEAN'][primeirotri['ASSUNTO CLEAN'].notnull()]\n",
        ")\n",
        "\n",
        "str_assuntoclean = str_assuntoclean.replace(' bairros ', ' bairro ')\n",
        "str_assuntoclean = str_assuntoclean.replace(' obras ', ' obra ')\n",
        "str_assuntoclean = str_assuntoclean.replace(' interdicoes ', ' interdicao ')\n",
        "\n",
        "lista_palavras_comuns = ['dmae', 'agua', 'esgoto', 'saneamento', 'basico', 'lixo', 'coleta', 'seletiva', 'cidade', 'bairro', 'local',\n",
        "                         'uberlandia', 'prefeitura', 'avenida', 'rua', 'servico', 'regiao', 'rede', 'iptu',\n",
        "                         'segunda', 'terça', 'quarta', 'quinta', 'sexta', 'sabado', 'domingo', 'feira',\n",
        "                        ]\n",
        "\n",
        "str_assuntoclean = ' '.join(x for x in str_assuntoclean.split() if x not in lista_palavras_comuns)\n",
        "str_assuntoclean = ' '.join(x for x in str_assuntoclean.split())\n",
        "tokens = nltk.word_tokenize(str_assuntoclean)\n",
        "frequencia = nltk.FreqDist(tokens)\n",
        "\n",
        "wordcloud = WordCloud(width=800, height=600, background_color='white', colormap='Dark2', mask=None, random_state=2022)\n",
        "wordcloud.generate_from_frequencies(frequencies=frequencia)\n",
        "plt.figure(figsize=(16, 12))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6TCHm0ND3NIv"
      },
      "outputs": [],
      "source": [
        "#Nuvem de palavras do 2º Trimestre de 2022\n",
        "segundotri = df[(df['DATA'] >= '2022-04-01') & (df['DATA'] <= '2022-06-30')]\n",
        "str_assuntoclean = ' '.join(\n",
        "    segundotri['ASSUNTO CLEAN'][segundotri['ASSUNTO CLEAN'].notnull()]\n",
        ")\n",
        "\n",
        "str_assuntoclean = str_assuntoclean.replace(' bairros ', ' bairro ')\n",
        "str_assuntoclean = str_assuntoclean.replace(' obras ', ' obra ')\n",
        "str_assuntoclean = str_assuntoclean.replace(' interdicoes ', ' interdicao ')\n",
        "\n",
        "lista_palavras_comuns = ['dmae', 'agua', 'esgoto', 'saneamento', 'basico', 'lixo', 'coleta', 'seletiva', 'cidade', 'bairro', 'local',\n",
        "                         'uberlandia', 'prefeitura', 'avenida', 'rua', 'servico', 'regiao', 'rede', 'iptu',\n",
        "                         'segunda', 'terça', 'quarta', 'quinta', 'sexta', 'sabado', 'domingo', 'feira',\n",
        "                        ]\n",
        "\n",
        "str_assuntoclean = ' '.join(x for x in str_assuntoclean.split() if x not in lista_palavras_comuns)\n",
        "str_assuntoclean = ' '.join(x for x in str_assuntoclean.split())\n",
        "tokens = nltk.word_tokenize(str_assuntoclean)\n",
        "frequencia = nltk.FreqDist(tokens)\n",
        "\n",
        "wordcloud = WordCloud(width=800, height=600, background_color='white', colormap='Dark2', mask=None, random_state=2022)\n",
        "wordcloud.generate_from_frequencies(frequencies=frequencia)\n",
        "plt.figure(figsize=(16, 12))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apPB6rUi3frv"
      },
      "outputs": [],
      "source": [
        "#Nuvem de palavras do 3º Trimestre de 2022\n",
        "terceirotri = df[(df['DATA'] >= '2022-07-01') & (df['DATA'] <= '2022-09-30')]\n",
        "str_assuntoclean = ' '.join(\n",
        "    terceirotri['ASSUNTO CLEAN'][terceirotri['ASSUNTO CLEAN'].notnull()]\n",
        ")\n",
        "\n",
        "str_assuntoclean = str_assuntoclean.replace(' bairros ', ' bairro ')\n",
        "str_assuntoclean = str_assuntoclean.replace(' obras ', ' obra ')\n",
        "str_assuntoclean = str_assuntoclean.replace(' interdicoes ', ' interdicao ')\n",
        "\n",
        "lista_palavras_comuns = ['dmae', 'agua', 'esgoto', 'saneamento', 'basico', 'lixo', 'coleta', 'seletiva', 'cidade', 'bairro', 'local',\n",
        "                         'uberlandia', 'prefeitura', 'avenida', 'rua', 'servico', 'regiao', 'rede', 'iptu',\n",
        "                         'segunda', 'terça', 'quarta', 'quinta', 'sexta', 'sabado', 'domingo', 'feira',\n",
        "                        ]\n",
        "\n",
        "str_assuntoclean = ' '.join(x for x in str_assuntoclean.split() if x not in lista_palavras_comuns)\n",
        "str_assuntoclean = ' '.join(x for x in str_assuntoclean.split())\n",
        "tokens = nltk.word_tokenize(str_assuntoclean)\n",
        "frequencia = nltk.FreqDist(tokens)\n",
        "\n",
        "wordcloud = WordCloud(width=800, height=600, background_color='white', colormap='Dark2', mask=None, random_state=2022)\n",
        "wordcloud.generate_from_frequencies(frequencies=frequencia)\n",
        "plt.figure(figsize=(16, 12))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5axMtZH43f2k"
      },
      "outputs": [],
      "source": [
        "#Nuvem de palavras do 4º Trimestre de 2022\n",
        "quartotri = df[(df['DATA'] >= '2022-10-01') & (df['DATA'] <= '2022-12-31')]\n",
        "str_assuntoclean = ' '.join(\n",
        "    quartotri['ASSUNTO CLEAN'][quartotri['ASSUNTO CLEAN'].notnull()]\n",
        ")\n",
        "\n",
        "str_assuntoclean = str_assuntoclean.replace(' bairros ', ' bairro ')\n",
        "str_assuntoclean = str_assuntoclean.replace(' obras ', ' obra ')\n",
        "str_assuntoclean = str_assuntoclean.replace(' interdicoes ', ' interdicao ')\n",
        "\n",
        "lista_palavras_comuns = ['dmae', 'agua', 'esgoto', 'saneamento', 'basico', 'lixo', 'coleta', 'seletiva', 'cidade', 'bairro', 'local',\n",
        "                         'uberlandia', 'prefeitura', 'avenida', 'rua', 'servico', 'regiao', 'rede', 'iptu',\n",
        "                         'segunda', 'terça', 'quarta', 'quinta', 'sexta', 'sabado', 'domingo', 'feira',\n",
        "                        ]\n",
        "\n",
        "str_assuntoclean = ' '.join(x for x in str_assuntoclean.split() if x not in lista_palavras_comuns)\n",
        "str_assuntoclean = ' '.join(x for x in str_assuntoclean.split())\n",
        "tokens = nltk.word_tokenize(str_assuntoclean)\n",
        "frequencia = nltk.FreqDist(tokens)\n",
        "\n",
        "wordcloud = WordCloud(width=800, height=600, background_color='white', colormap='Dark2', mask=None, random_state=2022)\n",
        "wordcloud.generate_from_frequencies(frequencies=frequencia)\n",
        "plt.figure(figsize=(16, 12))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Nuvem de palavras do 1º Trimestre de 2023\n",
        "primeirotri23 = df[(df['DATA'] >= '2023-01-01') & (df['DATA'] <= '2023-03-31')]\n",
        "str_assuntoclean = ' '.join(\n",
        "    primeirotri23['ASSUNTO CLEAN'][primeirotri23['ASSUNTO CLEAN'].notnull()]\n",
        ")\n",
        "\n",
        "str_assuntoclean = str_assuntoclean.replace(' bairros ', ' bairro ')\n",
        "str_assuntoclean = str_assuntoclean.replace(' obras ', ' obra ')\n",
        "str_assuntoclean = str_assuntoclean.replace(' interdicoes ', ' interdicao ')\n",
        "\n",
        "lista_palavras_comuns = ['dmae', 'agua', 'esgoto', 'saneamento', 'basico', 'lixo', 'coleta', 'seletiva', 'cidade', 'bairro', 'local',\n",
        "                         'uberlandia', 'prefeitura', 'avenida', 'rua', 'servico', 'regiao', 'rede', 'iptu',\n",
        "                         'segunda', 'terça', 'quarta', 'quinta', 'sexta', 'sabado', 'domingo', 'feira',\n",
        "                        ]\n",
        "\n",
        "str_assuntoclean = ' '.join(x for x in str_assuntoclean.split() if x not in lista_palavras_comuns)\n",
        "str_assuntoclean = ' '.join(x for x in str_assuntoclean.split())\n",
        "tokens = nltk.word_tokenize(str_assuntoclean)\n",
        "frequencia = nltk.FreqDist(tokens)\n",
        "\n",
        "wordcloud = WordCloud(width=800, height=600, background_color='white', colormap='Dark2', mask=None, random_state=2023)\n",
        "wordcloud.generate_from_frequencies(frequencies=frequencia)\n",
        "plt.figure(figsize=(16, 12))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8nMRhbXbEYJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Nuvem de palavras do 2º Trimestre de 2023\n",
        "segundotri23 = df[(df['DATA'] >= '2023-04-01') & (df['DATA'] <= '2023-06-30')]\n",
        "str_assuntoclean = ' '.join(\n",
        "    segundotri23['ASSUNTO CLEAN'][segundotri23['ASSUNTO CLEAN'].notnull()]\n",
        ")\n",
        "\n",
        "str_assuntoclean = str_assuntoclean.replace(' bairros ', ' bairro ')\n",
        "str_assuntoclean = str_assuntoclean.replace(' obras ', ' obra ')\n",
        "str_assuntoclean = str_assuntoclean.replace(' interdicoes ', ' interdicao ')\n",
        "\n",
        "lista_palavras_comuns = ['dmae', 'agua', 'esgoto', 'saneamento', 'basico', 'lixo', 'coleta', 'seletiva', 'cidade', 'bairro', 'local',\n",
        "                         'uberlandia', 'prefeitura', 'avenida', 'rua', 'servico', 'regiao', 'rede', 'iptu',\n",
        "                         'segunda', 'terça', 'quarta', 'quinta', 'sexta', 'sabado', 'domingo', 'feira',\n",
        "                        ]\n",
        "\n",
        "str_assuntoclean = ' '.join(x for x in str_assuntoclean.split() if x not in lista_palavras_comuns)\n",
        "str_assuntoclean = ' '.join(x for x in str_assuntoclean.split())\n",
        "tokens = nltk.word_tokenize(str_assuntoclean)\n",
        "frequencia = nltk.FreqDist(tokens)\n",
        "\n",
        "wordcloud = WordCloud(width=800, height=600, background_color='white', colormap='Dark2', mask=None, random_state=2023)\n",
        "wordcloud.generate_from_frequencies(frequencies=frequencia)\n",
        "plt.figure(figsize=(16, 12))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "q7NCIPqjErGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Nuvem de palavras do 3º Trimestre de 2023\n",
        "terceirotri23 = df[(df['DATA'] >= '2023-07-01') & (df['DATA'] <= '2023-09-30')]\n",
        "str_assuntoclean = ' '.join(\n",
        "    terceirotri23['ASSUNTO CLEAN'][terceirotri23['ASSUNTO CLEAN'].notnull()]\n",
        ")\n",
        "\n",
        "str_assuntoclean = str_assuntoclean.replace(' bairros ', ' bairro ')\n",
        "str_assuntoclean = str_assuntoclean.replace(' obras ', ' obra ')\n",
        "str_assuntoclean = str_assuntoclean.replace(' interdicoes ', ' interdicao ')\n",
        "\n",
        "lista_palavras_comuns = ['dmae', 'agua', 'esgoto', 'saneamento', 'basico', 'lixo', 'coleta', 'seletiva', 'cidade', 'bairro', 'local',\n",
        "                         'uberlandia', 'prefeitura', 'avenida', 'rua', 'servico', 'regiao', 'rede', 'iptu',\n",
        "                         'segunda', 'terça', 'quarta', 'quinta', 'sexta', 'sabado', 'domingo', 'feira',\n",
        "                        ]\n",
        "\n",
        "str_assuntoclean = ' '.join(x for x in str_assuntoclean.split() if x not in lista_palavras_comuns)\n",
        "str_assuntoclean = ' '.join(x for x in str_assuntoclean.split())\n",
        "tokens = nltk.word_tokenize(str_assuntoclean)\n",
        "frequencia = nltk.FreqDist(tokens)\n",
        "\n",
        "wordcloud = WordCloud(width=800, height=600, background_color='white', colormap='Dark2', mask=None, random_state=2023)\n",
        "wordcloud.generate_from_frequencies(frequencies=frequencia)\n",
        "plt.figure(figsize=(16, 12))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "S4mYix9hErSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-AUbTnU4Ky_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c6329c9-cd9c-4d64-f5c3-296d21bab3bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/mac_morpho.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import mac_morpho\n",
        "from nltk.tag import UnigramTagger\n",
        "nltk.download('punkt')\n",
        "nltk.download('mac_morpho')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "O_kiS6iW325r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uz5heAA84L3W"
      },
      "outputs": [],
      "source": [
        "# Leitura do arquivo de texto com as manchetes\n",
        "rawdata = open('//content/texto_manchetes_clean.txt', 'r', encoding='UTF-8').read()\n",
        "str_manchetes = rawdata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZIwMUhrI4daM"
      },
      "outputs": [],
      "source": [
        "#Palavras que serão substituídas na análise\n",
        "\n",
        "str_manchetes = str_manchetes.replace(' bairros ', ' bairro ')\n",
        "str_manchetes = str_manchetes.replace(' obras ', ' obra ')\n",
        "str_manchetes = str_manchetes.replace(' interdicoes ', ' interdicao ')\n",
        "str_manchetes = str_manchetes.replace(' novos ', ' novo ')\n",
        "str_manchetes = str_manchetes.replace(' nova ', ' novo ')\n",
        "str_manchetes = str_manchetes.replace(' novas ', ' novo ')\n",
        "str_manchetes = str_manchetes.replace(' obras ', ' obra ')\n",
        "str_manchetes = str_manchetes.replace(' falta ', ' faltar ')\n",
        "str_manchetes = str_manchetes.replace(' fez ', ' fazer ')\n",
        "str_manchetes = str_manchetes.replace(' faz ', ' fazer ')\n",
        "str_manchetes = str_manchetes.replace(' informou ', ' informa ')\n",
        "\n",
        "#Lista com as palavras mais comuns, que serão excluídas da análise\n",
        "lista_palavras_comuns = ['dmae', 'agua', 'esgoto', 'saneamento', 'basico', 'lixo', 'coleta', 'seletiva', 'cidade', 'bairro', 'local',\n",
        "                         'uberlandia', 'prefeitura', 'avenida', 'rua', 'servico', 'regiao', 'rede', 'iptu',\n",
        "                         'segunda', 'terça', 'quarta', 'quinta', 'sexta', 'sabado', 'domingo', 'feira',\n",
        "                         'telespectadores', 'telespectador', 'pinga', 'fogo', 'ouvinte', 'ouvintes',\n",
        "                        ]\n",
        "\n",
        "str_manchetes = ' '.join(x for x in str_manchetes.split() if x not in lista_palavras_comuns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9CTFZ5Ga4v5o"
      },
      "outputs": [],
      "source": [
        "#Tokenização das manchetes\n",
        "tokens = nltk.word_tokenize(str_manchetes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzCxftXs5SQE"
      },
      "outputs": [],
      "source": [
        "#Padronizando para etiquetar os tokens que não forem identificados como substantivo (N)\n",
        "tagger0 = nltk.DefaultTagger('N')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4jdk13m5T3Y"
      },
      "outputs": [],
      "source": [
        "#Etiquetando\n",
        "sentencas_treino = mac_morpho.tagged_sents()\n",
        "etiquetador = UnigramTagger(sentencas_treino, backoff=tagger0)\n",
        "\n",
        "etiquetado = etiquetador.tag(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HV68HIG85sof"
      },
      "outputs": [],
      "source": [
        "etiquetado[:20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ORUvHNT85tU5"
      },
      "outputs": [],
      "source": [
        "#Mostrando a frequência de cada etiqueta\n",
        "tags = [tag for (word, tag) in etiquetado]\n",
        "frequent_tags = nltk.FreqDist(tags)\n",
        "list(frequent_tags.keys())[0:20]\n",
        "frequent_tags.most_common(20)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotando o gráfico da frequência das etiquetas\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "most_common_tags = frequent_tags.most_common(10)\n",
        "most_common_tags = pd.Series(dict(most_common_tags))\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 4))\n",
        "all_plot = sns.barplot(x=most_common_tags.values, y=most_common_tags.index, ax=ax)\n",
        "\n",
        "for index, value in enumerate(most_common_tags.values):\n",
        "    all_plot.text(value + 0.1, index, str(value), ha='left', va='center', fontsize=8)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Fhjbf5MBj-I5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dW7T9GmB5_E5"
      },
      "outputs": [],
      "source": [
        "#Plotando o gráfico dos substantivos mais comuns\n",
        "words = [word for (word, tag) in etiquetado if tag == 'N']\n",
        "frequent_words = nltk.FreqDist(words)\n",
        "frequent_words.most_common(10)\n",
        "\n",
        "most_common_words = frequent_words.most_common(20)\n",
        "most_common_words = pd.Series(dict(most_common_words))\n",
        "\n",
        "sns.set(font_scale = 1.3)\n",
        "fig, ax = plt.subplots(figsize=(20,10))\n",
        "all_plot = sns.barplot(y=most_common_words.index, x=most_common_words.values, ax=ax, orient='h')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYoNcDH36G03"
      },
      "outputs": [],
      "source": [
        "#Plotando o gráfico dos verbos mais comuns\n",
        "words = [word for (word, tag) in etiquetado if tag == 'V']\n",
        "frequent_words = nltk.FreqDist(words)\n",
        "frequent_words.most_common(10)\n",
        "\n",
        "most_common_words = frequent_words.most_common(21)[1:20]\n",
        "most_common_words = pd.Series(dict(most_common_words))\n",
        "\n",
        "sns.set(font_scale = 1.3)\n",
        "fig, ax = plt.subplots(figsize=(20,10))\n",
        "all_plot = sns.barplot(y=most_common_words.index, x=most_common_words.values, ax=ax, orient='h')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "whsoE6--6RdL"
      },
      "outputs": [],
      "source": [
        "#Plotando o gráfico dos adjetivos mais comuns\n",
        "words = [word for (word, tag) in etiquetado if tag == 'ADJ']\n",
        "frequent_words = nltk.FreqDist(words)\n",
        "frequent_words.most_common(10)\n",
        "\n",
        "most_common_words = frequent_words.most_common(20)\n",
        "most_common_words = pd.Series(dict(most_common_words))\n",
        "\n",
        "sns.set(font_scale = 1.3)\n",
        "fig, ax = plt.subplots(figsize=(20,10))\n",
        "all_plot = sns.barplot(y=most_common_words.index, x=most_common_words.values, ax=ax, orient='h')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQz5nwBv6qB3"
      },
      "outputs": [],
      "source": [
        "#Palavras mais frequentes\n",
        "frequent_words.most_common(50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emxQmORw65Vh"
      },
      "outputs": [],
      "source": [
        "#Listando bigramas\n",
        "lista_bigramas = list(nltk.bigrams(tokens))\n",
        "print(\"Bigramas:\")\n",
        "print(lista_bigramas[0:15])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qh-PxrvB684D"
      },
      "outputs": [],
      "source": [
        "#Contagem de frequência dos bigramas\n",
        "frequent_bigramas = nltk.FreqDist(lista_bigramas)\n",
        "print(\"\\nBigramas mais comuns:\")\n",
        "print(frequent_bigramas.most_common(15))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jk4bxHx47BWB"
      },
      "outputs": [],
      "source": [
        "#Plotando bigramas\n",
        "most_common_bigrams = frequent_bigramas.most_common(15)\n",
        "most_common_bigrams = pd.DataFrame(most_common_bigrams, columns=['bigrama', 'quantidade'])\n",
        "\n",
        "sns.set(font_scale=1.3)\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "all_plot = sns.barplot(y=most_common_bigrams['bigrama'], x=most_common_bigrams['quantidade'], ax=ax, orient='h')\n",
        "plt.title('Bigramas')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Corrigindo bigramas\n",
        "stopwords = set(['d', 'agua'])\n",
        "\n",
        "filtered_bigrams = [(word1, word2) for word1, word2 in lista_bigramas if word1.lower() not in stopwords and word2.lower() not in stopwords]\n",
        "frequent_bigramas = nltk.FreqDist(filtered_bigrams)\n",
        "\n",
        "#Plotando bigramas corrigido\n",
        "most_common_bigrams = frequent_bigramas.most_common(15)\n",
        "most_common_bigrams = pd.DataFrame(most_common_bigrams, columns=['bigrama', 'quantidade'])\n",
        "\n",
        "sns.set(font_scale=1.3)\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "all_plot = sns.barplot(y=most_common_bigrams['bigrama'], x=most_common_bigrams['quantidade'], ax=ax, orient='h')\n",
        "plt.title('Bigramas')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nUlKeIwScrfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wuUOnrFC7d-m"
      },
      "outputs": [],
      "source": [
        "#Listando trigramas\n",
        "lista_trigramas = list(nltk.trigrams(tokens))\n",
        "print(\"\\nTrigramas:\")\n",
        "print(lista_trigramas[0:20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ach8Fra7k3m"
      },
      "outputs": [],
      "source": [
        "#Contagem de frequência dos trigramas\n",
        "frequent_trigramas = nltk.FreqDist(lista_trigramas)\n",
        "print(\"\\nTrigramas mais comuns:\")\n",
        "print(frequent_trigramas.most_common(15))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oyMeLvU87pfQ"
      },
      "outputs": [],
      "source": [
        "#Plotando trigramas\n",
        "most_common_trigrams = frequent_trigramas.most_common(15)\n",
        "most_common_trigrams = pd.DataFrame(most_common_trigrams, columns=['trigrama', 'quantidade'])\n",
        "\n",
        "sns.set(font_scale=1.3)\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "all_plot = sns.barplot(y=most_common_trigrams['trigrama'], x=most_common_trigrams['quantidade'], ax=ax, orient='h')\n",
        "plt.title('Trigramas')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Corrigindo trigramas\n",
        "stopwords = set(['d', 'agua'])\n",
        "\n",
        "filtered_trigramas = [(word1, word2, word3) for word1, word2, word3 in lista_trigramas if word1.lower() not in stopwords and word2.lower() not in stopwords and word3.lower() not in stopwords]\n",
        "frequent_trigramas = nltk.FreqDist(filtered_trigramas)\n",
        "\n",
        "#Plotando trigramas corrigido\n",
        "most_common_trigrams = frequent_trigramas.most_common(15)\n",
        "most_common_trigrams = pd.DataFrame(most_common_trigrams, columns=['trigrama', 'quantidade'])\n",
        "\n",
        "sns.set(font_scale=1.3)\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "all_plot = sns.barplot(y=most_common_trigrams['trigrama'], x=most_common_trigrams['quantidade'], ax=ax, orient='h')\n",
        "plt.title('Trigramas')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gXQ7E4bfdyq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXyHhIBqlySu"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pprint import pprint\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9n2bo7Bblzba"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "import spacy\n",
        "import logging\n",
        "import warnings\n",
        "from gensim import corpora\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import LdaModel, CoherenceModel\n",
        "from gensim.models.ldamulticore import LdaMulticore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qnRL-J5Ll4Fr"
      },
      "outputs": [],
      "source": [
        "!python -m spacy download pt_core_news_sm\n",
        "\n",
        "nlp = spacy.load('pt_core_news_sm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6mfmWRiqpwM"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ntc5v6skq0w1"
      },
      "outputs": [],
      "source": [
        "!pip install pyLDAvis==2.1.2\n",
        "import pyLDAvis.gensim as gensimvis\n",
        "import pyLDAvis\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud, STOPWORDS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8eyH3gwxrCpL"
      },
      "outputs": [],
      "source": [
        "#Carregando dados do arquivo .csv\n",
        "df_noticias = pd.read_csv('/content/dados_clean.csv')\n",
        "df_noticias.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bu8mVEbSrTrF"
      },
      "outputs": [],
      "source": [
        "#Extraindo as notícias da coluna 'ASSUNTO CLEAN'\n",
        "list_noticias = df_noticias['ASSUNTO CLEAN'].values.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGfA6Zw9rl1D"
      },
      "outputs": [],
      "source": [
        "list_noticias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "liamJAbjrkrF"
      },
      "outputs": [],
      "source": [
        "#Aplicando substituições nas palavras comuns\n",
        "for index, x in enumerate(list_noticias):\n",
        "    if isinstance(x, str):\n",
        "        list_noticias[index] = x.replace(' bairros ', ' bairro ') \\\n",
        "                                 .replace(' obras ', ' obra ') \\\n",
        "                                 .replace(' interdicoes ', ' interdicao ') \\\n",
        "                                 .replace(' falta ', ' faltar ') \\\n",
        "                                 .replace(' fez ', ' fazer ') \\\n",
        "                                 .replace(' faz ', ' fazer ') \\\n",
        "                                 .replace(' informou ', ' informa ') \\\n",
        "                                 .replace(' novos ', ' novo ') \\\n",
        "                                 .replace(' nova ', ' novo ') \\\n",
        "                                 .replace(' novas ', ' novo ')\n",
        "\n",
        "#Lista com as palavras mais comuns, que serão excluídas da análise\n",
        "lista_palavras_comuns = ['dmae', 'agua', 'esgoto', 'saneamento', 'basico', 'lixo', 'coleta', 'seletiva', 'cidade', 'bairro', 'local',\n",
        "                         'uberlandia', 'prefeitura', 'avenida', 'rua', 'servico', 'regiao', 'rede', 'iptu',\n",
        "                         'segunda', 'terça', 'quarta', 'quinta', 'sexta', 'sabado', 'domingo', 'feira',\n",
        "                         'telespectadores', 'telespectador', 'pinga', 'fogo', 'ouvinte', 'ouvintes',\n",
        "                        ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7tVWKXrrqw9"
      },
      "outputs": [],
      "source": [
        "#Removendo palavras comuns\n",
        "for index, x in enumerate(list_noticias):\n",
        "    x = str(x)\n",
        "    list_noticias[index] = ' '.join(word for word in x.split() if word not in lista_palavras_comuns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1bDDlYJsV5c"
      },
      "outputs": [],
      "source": [
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
        "\n",
        "#Tokenização das notícias\n",
        "dados_palavras = list(sent_to_words(list_noticias))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KAaQlh8mslhJ"
      },
      "outputs": [],
      "source": [
        "#Criando um dicionário e corpus para o LDA\n",
        "id2word = corpora.Dictionary(dados_palavras)\n",
        "texts = dados_palavras\n",
        "corpus = [id2word.doc2bow(text) for text in dados_palavras]\n",
        "print(corpus[:1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rIJX0KJ2svEQ"
      },
      "outputs": [],
      "source": [
        "#Construindo o modelo LDA\n",
        "lda_model = LdaModel(corpus=corpus,\n",
        "                     id2word=id2word,\n",
        "                     num_topics=4,\n",
        "                     random_state=2022,\n",
        "                     update_every=1,\n",
        "                     chunksize=100,\n",
        "                     passes=10,\n",
        "                     alpha='auto',\n",
        "                     per_word_topics=True)\n",
        "\n",
        "doc_lda = lda_model[corpus]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c79WpYfhs1BN"
      },
      "outputs": [],
      "source": [
        "#Exibindo tópicos\n",
        "pprint(lda_model.print_topics())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Verificando a coerência do modelo\n",
        "# Calculando a perplexidade (quanto menor, melhor)\n",
        "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))\n",
        "\n",
        "# Calculando o score de coerência (quanto maior, melhor)\n",
        "coherence_model_lda = CoherenceModel(model=lda_model, texts=dados_palavras, dictionary=id2word, coherence='c_v')\n",
        "coherence_lda = coherence_model_lda.get_coherence()\n",
        "print('\\nCoherence Score: ', coherence_lda)"
      ],
      "metadata": {
        "id": "fjC3EmadI2Ob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualização interativa do modelo LDA\n",
        "vis_data = gensimvis.prepare(lda_model, corpus, id2word)\n",
        "pyLDAvis.display(vis_data)"
      ],
      "metadata": {
        "id": "2Z_p-1XmnaRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Salvar a visualização como um arquivo HTML\n",
        "pyLDAvis.save_html(vis_data, 'lda_visualization.html')"
      ],
      "metadata": {
        "id": "V-ggV6g1nbNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotando coerência e perplexidade para diferentes quantidades de tópicos\n",
        "from gensim.models import CoherenceModel\n",
        "\n",
        "topic_range = range(3, 8)\n",
        "coherence_scores = []\n",
        "perplexity_scores = []\n",
        "\n",
        "for num_topics in topic_range:\n",
        "    lda_model = LdaModel(corpus=corpus, id2word=id2word, num_topics=num_topics)\n",
        "\n",
        "    #Coerência\n",
        "    coherence_model = CoherenceModel(model=lda_model, texts=texts, dictionary=id2word, coherence='c_v')\n",
        "    coherence_score = coherence_model.get_coherence()\n",
        "    coherence_scores.append(coherence_score)\n",
        "\n",
        "    #Perplexidade\n",
        "    perplexity = lda_model.log_perplexity(corpus)\n",
        "    perplexity_scores.append(perplexity)\n",
        "\n",
        "    #Mostra os valores para cada número de tópicos\n",
        "    print(f\"Número de Tópicos: {num_topics}, Coerência: {coherence_score}, Perplexidade: {perplexity}\")"
      ],
      "metadata": {
        "id": "gtqnl9q3L5mg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Atualizando o modelo LDA com o número ideal de tópicos\n",
        "optimal_num_topics = 4 #Substituir pela quantidade escolhida\n",
        "original_lda_model = LdaModel(corpus=corpus, id2word=id2word, num_topics=optimal_num_topics)\n",
        "topics_original = original_lda_model.show_topics(formatted=False)\n",
        "new_lda_model = LdaModel(corpus=corpus, id2word=id2word, num_topics=optimal_num_topics, alpha='auto', eta='auto')\n",
        "topics_new = new_lda_model.show_topics(formatted=False)\n",
        "\n",
        "print(\"Tópicos do modelo original:\")\n",
        "print(topics_original)\n",
        "\n",
        "print(\"\\nTópicos do novo modelo:\")\n",
        "print(topics_new)"
      ],
      "metadata": {
        "id": "09xoTiR0MplP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Exibindo tópicos do modelo atualizado\n",
        "pprint(new_lda_model.print_topics())"
      ],
      "metadata": {
        "id": "UVrBMy7-cwsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDWhs-P8tKeE"
      },
      "outputs": [],
      "source": [
        "# Visualização interativa do modelo LDA atualizado\n",
        "lda_model = LdaModel(corpus=corpus, id2word=id2word, num_topics=optimal_num_topics, alpha='auto', eta='auto')\n",
        "lda_vis_data = gensimvis.prepare(lda_model, corpus, id2word)\n",
        "pyLDAvis.display(lda_vis_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TEO-pLXBujW5"
      },
      "outputs": [],
      "source": [
        "#Salvar a visualização como um arquivo HTML\n",
        "pyLDAvis.save_html(vis_data, 'lda_visualization_atualizado.html')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}